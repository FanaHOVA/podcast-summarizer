{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "712fdefe-b4a9-4244-9c37-74cb3a179187",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3417.91s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nba_api in /opt/homebrew/lib/python3.10/site-packages (1.1.14)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.22.2 in /opt/homebrew/lib/python3.10/site-packages (from nba_api) (1.24.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from nba_api) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests->nba_api) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests->nba_api) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->nba_api) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->nba_api) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3424.11s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.10/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3430.36s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/homebrew/lib/python3.10/site-packages (0.0.74)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (1.10.4)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (1.24.1)\n",
      "Requirement already satisfied: SQLAlchemy<2,>=1 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (1.4.46)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (2.28.2)\n",
      "Requirement already satisfied: PyYAML<7,>=6 in /opt/homebrew/lib/python3.10/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/homebrew/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/homebrew/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/homebrew/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/homebrew/lib/python3.10/site-packages (from pydantic<2,>=1->langchain) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/homebrew/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/homebrew/lib/python3.10/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3436.33s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yt-dlp in /opt/homebrew/lib/python3.10/site-packages (2023.1.6)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.10/site-packages (from yt-dlp) (2022.12.7)\n",
      "Requirement already satisfied: pycryptodomex in /opt/homebrew/lib/python3.10/site-packages (from yt-dlp) (3.17)\n",
      "Requirement already satisfied: brotli in /opt/homebrew/lib/python3.10/site-packages (from yt-dlp) (1.0.9)\n",
      "Requirement already satisfied: mutagen in /opt/homebrew/lib/python3.10/site-packages (from yt-dlp) (1.46.0)\n",
      "Requirement already satisfied: websockets in /opt/homebrew/lib/python3.10/site-packages (from yt-dlp) (10.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3442.27s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in /opt/homebrew/lib/python3.10/site-packages (20230124)\n",
      "Requirement already satisfied: torch in /opt/homebrew/lib/python3.10/site-packages (from openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.10/site-packages (from openai-whisper) (4.64.1)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.10/site-packages (from openai-whisper) (1.24.1)\n",
      "Requirement already satisfied: more-itertools in /opt/homebrew/lib/python3.10/site-packages (from openai-whisper) (9.0.0)\n",
      "Requirement already satisfied: transformers>=4.19.0 in /opt/homebrew/lib/python3.10/site-packages (from openai-whisper) (4.26.0)\n",
      "Requirement already satisfied: ffmpeg-python==0.2.0 in /opt/homebrew/lib/python3.10/site-packages (from openai-whisper) (0.2.0)\n",
      "Requirement already satisfied: future in /opt/homebrew/lib/python3.10/site-packages (from ffmpeg-python==0.2.0->openai-whisper) (0.18.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.19.0->openai-whisper) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.19.0->openai-whisper) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.19.0->openai-whisper) (0.12.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.19.0->openai-whisper) (3.9.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.19.0->openai-whisper) (2.28.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.19.0->openai-whisper) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.19.0->openai-whisper) (23.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.10/site-packages (from torch->openai-whisper) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers>=4.19.0->openai-whisper) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers>=4.19.0->openai-whisper) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers>=4.19.0->openai-whisper) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests->transformers>=4.19.0->openai-whisper) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3448.29s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/homebrew/lib/python3.10/site-packages (0.21.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3454.28s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/homebrew/lib/python3.10/site-packages (0.26.4)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.10/site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/homebrew/lib/python3.10/site-packages (from openai) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.20->openai) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.10/site-packages (from aiohttp->openai) (1.8.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3460.41s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.1.2-cp310-cp310-macosx_11_0_arm64.whl (698 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.3/698.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /opt/homebrew/lib/python3.10/site-packages (from tiktoken) (2022.10.31)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/homebrew/lib/python3.10/site-packages (from tiktoken) (2.28.2)\n",
      "Collecting blobfile>=2\n",
      "  Downloading blobfile-2.0.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pycryptodomex~=3.8 in /opt/homebrew/lib/python3.10/site-packages (from blobfile>=2->tiktoken) (3.17)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /opt/homebrew/lib/python3.10/site-packages (from blobfile>=2->tiktoken) (1.26.14)\n",
      "Collecting lxml~=4.9\n",
      "  Downloading lxml-4.9.2.tar.gz (3.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock~=3.0 in /opt/homebrew/lib/python3.10/site-packages (from blobfile>=2->tiktoken) (3.9.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Building wheels for collected packages: lxml\n",
      "  Building wheel for lxml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lxml: filename=lxml-4.9.2-cp310-cp310-macosx_12_0_arm64.whl size=1676870 sha256=bb80aa157ed2d1643295acfe6292a5e6d0f058b03806fe08b9bbb02341002012\n",
      "  Stored in directory: /Users/alessiofanelli/Library/Caches/pip/wheels/5a/51/0e/95b4a6ddee4a616530c36aeb03dafb5e04183756d9973a7d5d\n",
      "Successfully built lxml\n",
      "Installing collected packages: lxml, blobfile, tiktoken\n",
      "Successfully installed blobfile-2.0.1 lxml-4.9.2 tiktoken-0.1.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nba_api\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install langchain\n",
    "!{sys.executable} -m pip install yt-dlp\n",
    "!{sys.executable} -m pip install openai-whisper\n",
    "!{sys.executable} -m pip install python-dotenv\n",
    "!{sys.executable} -m pip install openai\n",
    "!{sys.executable} -m pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d72b24e-45cf-45fc-b2ca-a947f3e1022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=57OU18cogJI\n",
      "[youtube] 57OU18cogJI: Downloading webpage\n",
      "[youtube] 57OU18cogJI: Downloading android player API JSON\n",
      "[info] 57OU18cogJI: Downloading 1 format(s): 140\n",
      "[download] Destination: ./tmp/foo_StrictlyVC in conversation with Sam Altman, part one-57OU18cogJI.m4a\n",
      "[download] 100% of   19.03MiB in 00:00:00 at 22.37MiB/s  \n",
      "[FixupM4a] Correcting container of \"./tmp/foo_StrictlyVC in conversation with Sam Altman, part one-57OU18cogJI.m4a\"\n",
      "[ExtractAudio] Not converting audio ./tmp/foo_StrictlyVC in conversation with Sam Altman, part one-57OU18cogJI.m4a; file is already in target format m4a\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "\n",
    "URLS = ['https://www.youtube.com/watch?v=57OU18cogJI']\n",
    "\n",
    "ydl_opts = {\n",
    "    'format': 'm4a/bestaudio/best',\n",
    "    'postprocessors': [{\n",
    "        'key': 'FFmpegExtractAudio',\n",
    "        'preferredcodec': 'm4a',\n",
    "    }],\n",
    "    'outtmpl': './tmp/foo_%(title)s-%(id)s.%(ext)s'\n",
    "}\n",
    "\n",
    "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "    error_code = ydl.download(URLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faa60d9e-147d-4914-b94e-b6ce864ca384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/whisper/transcribe.py:78: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import os\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "podcasts_to_analyze = []\n",
    "\n",
    "for file in os.listdir(\"./tmp\"):\n",
    "    # Skip if the file is not a video or audio file\n",
    "    if not file.endswith(\".m4a\"):\n",
    "        continue\n",
    "    \n",
    "    file_path = os.path.join(\"./tmp\", file)\n",
    "    result = model.transcribe(file_path)\n",
    "    podcasts_to_analyze.append(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "baf49360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\" So appreciate it. Thanks for having me. So Sam, you also very nicely came to an event that happened to be right here three years ago. Yeah, I remember. What's been happening? Um, open the eye mostly. Yeah, it's just taken up a lot of my time, but it's super great. I think we're doing a lot of stuff we're really proud of. I mean, I'm half kidding, obviously. But you know, you've been at the center of the startup conversation for almost 10 years since you had taken over as the president of a way. Combinator. Wow, that has been 10 years. Yeah, or something like that. Almost. Paul Graham once said that Sam is exceedingly good at becoming powerful, which I think is very funny. You are now at the center of the national conversation. I mean, to an extent that I think, you know, sort of has taken us all collectively back. I'm just wondering, how is that for you? Are you like springing out of bed? Or are you like waking up, dreading, you know, the headlines? I saw that you had posted. I don't read the news. I honestly, I think if I, and I don't really do stuff like this much, at all, I think if I could like just stop trolling on Twitter, which I really love for some reason I can't explain. I think I would just like really accomplish my goal of not, you know, being very quiet. But Twitter is fun. Well, I saw that you posted a burping emoji yesterday. Without comment, and I wondered if that had anything to do with. No, I had a bad morning for like extremely pedestrian reasons. Like my house flooded. I got in a car crash. Oh, God. You know, I'm allowed to use Twitter like a regular person. Sure, absolutely. Oh, well, I mean, we're not tired of talking about Twitter, but any thoughts about, you know, your friend is running it. How do you think things are going? I think it's going to be fine. I remember that night where everyone was like, get your tweets off right now. Say you're goodbyes. I heard from my like, you know, brothers, roommates, fathers, uncles, whatever, that like, it's all going to melt down to night. And it's all over. And, you know, it's still here. Right. I think it's good. Because they went to Mass and then they saw what the alternative was. I think it's, I think it's, I would be making some different decisions. But also I have like unbelievable respect for Elon. I wouldn't bet against him. And I think it's most likely going to be fine. Well, you know, there are a lot of reporters who have been generating the coverage that we've all been reading. So I was going to say if there's anything that you want to confirm or correct, I'm sure they would be delighted. But in the meantime, you know, strictly VC is really about investors and startup founders. And I think in the same way that people are very interested in OpenAI because of your involvement. They are really interested in your work as an investor. So I thought just to start, if it's okay with you, we could kind of talk a little bit about that aspect of your life. Starting with... Sure, but can I correct one thing? Sure. I don't think people are interested in OpenAI because of my involvement. I think OpenAI has managed to pull together the most talent-dense researchers and engineers in the field of AI who have done just like incredible work. And I think what people are interested in is like OpenAI from a cold start a few years ago has managed to do this thing that I think is going to be incredibly important to the next many decades at least. Many decades at least of society and how we all live our lives and what we do and what's possible. And I think it's going to be tremendously good. But the reason people, I think, are interested in OpenAI is because of the work that those people do. We've managed to make a research lab that has been able to deliver some cool stuff and I think we'll deliver a lot more cool things. So just wanted to... Absolutely. And you have to... you are one of the best storytellers in Silicon Valley, possibly the business, I think that counts for a lot. So I'm going to argue with you there. But how many investments do you have, large, you know, like all together? Active. I'm trying to get a sense for... I mean counting all the YC ones, like a few thousand personal ones... I would guess 400. Wow, really? Well, I've been doing this for a long time. Yeah, absolutely. I mean, and everyone's one I see, like a really gigantic deal. What makes a SAM Altman deal? Um... I try to just do things that I'm interested in at this point. One of the things I have realized is all of the companies I think I have added a lot of value to are the ones that I sort of like think about in my free time on a hike or whatever. And then techs the founders say, hey, I have this idea for you. And I have learned kind of like what those are and the ones that are not. I think like every founder deserves an investor who is like going to think about them while they're hiking. And so I've tried to like hold myself to the stuff that I really love, which tends to be like the hard tech years of R&D, capital intensive, or like, sort of like risky research. And then, but if it works, it really works. Well, one of the investments that I think is so interesting and obviously it's very interesting to you too is Helian. Yeah. That company announced you, so you've been investing in Helian since 2015. Yeah. But it announced a $500 million investment last year and you participated, you wrote them a $375 million check, which I think probably surprised people because there's not that many people who can write a $375 million check. Or not that many people who would like do it into like one risky fusion company. Well, I mean, I wanted to ask, so you mentioned your icy companies and I guess in the aggregate maybe that's the answer. I just wondered, like, which have been your most successful investments to date? I mean, probably on a multiple spaces, definitely on a multiple spaces, Stripe. And also, I think that was like my second investment ever. So it seemed like a lot easier. This was also a time when evaluations were different. It was great. But probably that one on a multiple spaces. But then, you know, I've been doing this for like 17 years, I guess. So there's been a lot of really good ones. And like super grateful to have been in Silicon Valley at what was such like a magical time. Helian is like more than an investment. I mean, that's the other thing besides opening an iceman, a lot of time on. And just super excited about what's going to happen there. So tell me about it because I don't really, I mean, I don't understand this. I saw what happened at Lawrence Livermore last month. Yeah. I wondered what you thought of that. It's a very different approach. Maybe if you can sort of explain since you're the expert. Super happy for them. I think it's like a very cool scientific result. As they themselves said, I don't think it'll be commercially relevant. And that's what I'm excited about. Not sort of getting fusion to work in a lab, although that is cool too. But building a system that will work at a super low cost. So if you look at the previous energy transitions, if you can get the cost of a new form of energy down, it can like take over everything else in a couple of decades, just phenomenally fast. And then also a system where we can create enough energy and enough reliable energy, both in terms of the machines not breaking and also not having the intermittance or the need for storage of solar or wind or something like that. If we can create enough for Earth in like 10 years. And I think that's actually the hardest challenge that helium faces. As we sketch out what it takes operationally to do that, to replace all the current generative capacity on Earth with fusion and to do it really fast, and to think about what it really means to build a factory that's capable of like putting out two of these machines a day for a decade. That's really hard. But also a super fun problem. So I think there will be, I'm very happy there's a fusion race. I think that's great. I'm also very happy solar and batteries are getting so cheap. But I think what will matter is who can deliver energy, the cheapest and enough of it. And again, just knowing only what I read on a superficial way, why is helium's approach to your mind superior than what these, what dozens of nations are working on in the South of France? Yeah. Well, that thing, I think probably will work either. But what I was just saying earlier, I think it will be commercially irrelevant. They also think it will be commercially irrelevant. The thing that is so exciting to me about helium is that it's a simple machine that is at affordable cost and a reasonable size. There's a bunch of different elements of it than like the giant Tocomax. But one that is very cool is what comes out of the reaction is charge particles, not heat. Almost all other sort of like a coal plant or a natural gas plant or whatever makes heat drive the steam turbine. That's what it does. Hewie-on-mix charge particles, which push back on the magnet and drive an electrical current down a wire. There's no heat cycle at all. And so it can be a much simpler, much more efficient system. And that, I think, is like missed out of the whole discussion on fusion. But really great. It also means we don't have to deal with like much nuclear material. We don't ever have like dangerous waste or even a dangerous system. You could like touch it pretty shortly after it turns off. And I mean, so I know it's building a big facility right now. Has it proven its thesis? We will have more to share there shortly. Okay. Well, after talking to you last time a few years ago and looking back on our conversation where I was like, ah, Sam, I know everything that you said was going to happen is happening. I take you more seriously than I did and should have. There's a long way to go, but thank you. So I also wanted to ask about some of your other investments, one of which I think is really interesting, Hermias. Yeah. So Hermias is interesting for a few reasons. Hermias is a supersonic jet company that wants to go at like five times the speed of sound. So that's cool. Also a big investment from you, I think. It was like a hundred million dollar round that you led. But also you were involved with a competitor for a while. You're on the board of Boom Supersonic, whose CEO has also participated in a strictly VC event. So just wondering why being choices. Not at all change for some sense. Boom is a different technology and it's like a mock two-ish airplane instead of a mock five-ish airplane. Hermias is like a ramjet technology that has very different characteristics. But I think there will be, like it's a huge market. I think there will be multiple needs. I think these are pretty very different approaches. And my general approach is if there's like an area that I think is really important, like energy for example, I try to fund the best fusion and the best fusion company I can. They're competitive in the sense that they're both trying to make cheap energy, but like we desperately need more cheap energy. It's a huge market. I think they can both work. I wouldn't have funded two like exact same approach airplane companies. But I think these are like very different. And also, you know, you are somebody who thinks about sort of second order effects. When you think about Hermias, I mean, first of all, I guess is it climate friendly? And second, what are the impacts I guess good and bad of us traveling around the world much faster than we do right now? Part of my theory is that, so it's not climate friendly if it's using current aviation fuel. I think even if something like fusion doesn't happen, there's a pretty good move to sustainable aviation fuel. And you know, at some point, we'll be all using that anyway. If something like fusion does work, it will so change the dynamics of what's possible in terms of our ability to create things like aviation fuel easily or capture carbon out of the atmosphere that I am confident enough in that working, that things I was much more nervous about doing a few years ago, creating faster airplanes that will increase the need for fuel because you have to burn a lot more fuel to go even a little bit faster. I'm sort of much more open to. In terms of like the benefits of traveling fast, I think human history is like a pretty good. There's like quite good evidence that when we are able to travel faster and more conveniently good things happen. Where commerce happens, more innovation happens, I think people develop much more empathy. Certainly the time I have spent traveling around the world and seeing very different things, very different problems, meeting very different people have been super formative for me and I think more of that's a good thing. I guess one downside is the spread of disease happens faster, but that's... Yes, although I think blaming faster planes for the spread of disease rather than the incompetence of governments and insufficient funding for pandemic responses, sort of the wrong way to go about it. What about World Coin? That was a strange one and that one was not received well by the media. We probably didn't understand it, but... You can't win them all. Wait, can I read the headline in Bloomberg? Sam Alvin wants to scan your eyeball in exchange for cryptocurrency. What is going on with that company? Is it still working on it and should we be scared? I am... yeah, like I think that's... they'll have more to share soon. I am like a co-founder, I'm on the board, I'm not day to day involved, but I think super highly of them. I think the press cycle... it came from a leak the company hadn't... It was not ready to tell its story yet, that was unfortunate. I think they'll do it soon and I think it will go over well. I think the need... so I try to think about not any individual company but sort of where the world is going to co-evolve. I think at this point the need for systems that provide proof of personhood and the need for new experiments with well-3 distribution and global governance of systems like say, an AGI is higher. I'm very glad this experiment is running. I'd like to see many more. I think to me, personally and again, people will have different opinions and they'll do what they want. But the amount of privacy you give up to like use Facebook or something versus the amount of privacy you give up for like a scan of your retina and nothing else, like I'd much rather have the latter. And many people won't want that and that's fine. But I think more experiments about what we can do, what problems we can solve with technology in this sort of new world, like great to try that stuff. I think it's a phenomenal team. I think they've got a great product. I'm excited for the launch. When is that? I don't know exactly, but pretty soon, like months. And you're a co-founder, but you're obviously not very involved. One of the investors, I just happened to notice with Sam Bankman-Freed, who's also like you and Emily. I did not know that. Very interesting. I really didn't. He's personally an investor in the company. I mean, according to some report. No, I didn't know that. Do you know him? We met briefly, very briefly once only. Okay, so not enough to form an opinion. Okay. Scratch that. I have questions. Not good. I wanted to ask about then crypto more broadly. You have a smattering of crypto investments. I don't know how interested you are if this is like friends that you've backed. Honestly, not super interested. I'm interested in World Coin, not because it's crypto, but because I think it's an interesting... It's an interesting attempt to use technology to solve something that is beyond what even governments in the world can effectively do. And I think if we can use technology, any technology, to experiment with global UBI instead of what one country could do, I'd be very happy to see what happens there, but that's not really about any particular technology. I think crypto is just like a way that we should try implementing that. And we should, again, try lots and lots of other things. So we don't need a new web. We don't need new infrastructure. We don't need decentralization. This is like one of the things that makes me feel really old and out of touch. I've never quite understood that. I love the spirit of the Web 3 people, but I don't intuitively feel why we need it. That's great. That's a relief, because I think a lot of people feel the same way. I want to move on soon, but another company that I think is so interesting, and again, these are also different and ambitious, is Conception, which is a startup pursuing what's called Invitro Gamer to Genesis, which refers to turning adult cells into gametes, sperm or egg cells. So, I mean, is this fantasy what makes you think that artificial eggs is... It's not possible. Fantasy, you know, there was a recent paper. They really truly have this not working in mice. Obviously working in mice is very different than working in humans. We've learned that lesson many times. But it seems to me like it should work at some point. It's not soon. There's a gigantic amount of work left to do. But I think what's happening in biotech in general is just tremendously exciting now. I think it's like kind of in the shadow of AI, which is taken over so much of the mind share. But I think the next five, seven years of biotech progress is going to be remarkable. And yeah, I think like if we do this again three years later, that one in particular will seem like, oh yeah, that's going to work. And I think things that are even further out there like human life extension or whatever will also seem like, yeah, maybe that's going to work. That's phenomenal. I guess there's obviously a lot of overlap, which is why you think that these things are going to work. Yeah, I think a lot of these things have these weirdly synergistic effects with each other. But even without that, I would just say biotech on its own has made quite a lot of progress. There's so much interesting stuff going on. I didn't reviewed a founder recently who's trying to extend the life of women's overuse, which I also think is really interesting. Okay, before we move on, you have been investing for 20 years. You were the president of Y-combinator for something like five or six years. You have your successor, Jeff Ralston, just left, Gary Tankiman. You had told Tad Friend once that when a CEO takes over a company, they have to kind of re-found, I think, was the word use the company. Do you think Gary's got to do anything differently? Gary's awesome. I think Gary will do a lot of things differently and be wildly successful at it. We're in a very different world and market now. I got to run, I said this once as somebody, I got to run Y-C at the time where any idiot would have been wildly successful. That was great. That was a lot of fun. And then the last couple of years, I think, were really hard. But now, when everything is bombed out, I think it's a wonderful opportunity. I think Y-C can really remake itself. And I think Gary is an incredible leader to do that. And at the time when all of the tourists are leaving and all of the people who are starting startups or raising their seed fund or whatever, because it was like the fashionable thing to do are leaving. This is when the great value gets created. This is like the best time to start a startup in many, many, many years. So I'm very excited for him.\"]\n"
     ]
    }
   ],
   "source": [
    "print(podcasts_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3081ef70-f7c3-47d4-aefc-e1cd2b01aa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "Read the transcript of the podcast below:\n",
    " {podcast_contents}\n",
    "\n",
    "Summarize the topics discussed in the podcast using bullet points:\n",
    "\"\"\"\n",
    "\n",
    "podcast_summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"podcast_contents\"],\n",
    "    template=template,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adcd4686-0856-4392-923d-dce6d5834e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "When I listen to a podcast, I take notes on the main talking points of the hosts. I divide it in sections based on topics discussed. \n",
    "If the host mentions a specific technology or product, I note that in double brackets like this: [[artificial intelligence]].\n",
    "\n",
    "These are the notes from the last podcast I listened to:\n",
    "\n",
    "{podcast_notes}\n",
    "\n",
    "Write a {words_count} words summary of the notes.\n",
    "\"\"\"\n",
    "\n",
    "notes_summary_template = PromptTemplate(\n",
    "    input_variables=[\"podcast_notes\", \"words_count\"],\n",
    "    template=template,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9472a7e-ad26-469b-bd99-2904a4ec730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "When I take notes for a podcast, I like to also write twitter threads to share them. Each tweet should end saying how far we are in the thread; if it's a 5 tweets thread, the first tweet should end with (1/5), the second one with (2/5), etc.\n",
    "\n",
    "The tweets have to be easy to read and catch people's attention. Each of them should include an emoji.\n",
    "\n",
    "These are the notes from my last podcast:\n",
    "\n",
    "{podcast_notes}\n",
    "\n",
    "Create a twitter thread for it.\n",
    "\"\"\"\n",
    "\n",
    "twitter_thread_template = PromptTemplate(\n",
    "    input_variables=[\"podcast_notes\"],\n",
    "    template=template,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c61a4bd2-9c89-4a8e-bf3d-792cb97369c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 4097 tokens, however you requested 4671 tokens (4415 in your prompt; 256 for the completion). Please reduce your prompt; or completion length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m results \u001b[39m=\u001b[39m []  \n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts:\n\u001b[0;32m---> 19\u001b[0m   subset \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39;49mrun(text)\n\u001b[1;32m     20\u001b[0m   results\u001b[39m.\u001b[39mappend(subset)\n\u001b[1;32m     22\u001b[0m all_results\u001b[39m.\u001b[39mappend(results\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain/chains/base.py:180\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    179\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 180\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    183\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain/chains/base.py:155\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m    157\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_outputs(outputs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain/chains/base.py:152\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    147\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    148\u001b[0m     inputs,\n\u001b[1;32m    149\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    151\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[1;32m    153\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain/chains/llm.py:86\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 86\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply([inputs])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain/chains/llm.py:77\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[1;32m     76\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(input_list)\n\u001b[1;32m     78\u001b[0m     outputs \u001b[39m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m     \u001b[39mfor\u001b[39;00m generation \u001b[39min\u001b[39;00m response\u001b[39m.\u001b[39mgenerations:\n\u001b[1;32m     80\u001b[0m         \u001b[39m# Get the text of the top generated string.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain/chains/llm.py:72\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mIf `stop` is present in any inputs, should be present in all.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         )\n\u001b[1;32m     71\u001b[0m     prompts\u001b[39m.\u001b[39mappend(prompt)\n\u001b[0;32m---> 72\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m     73\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain/llms/base.py:79\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m     81\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain/llms/base.py:76\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m     73\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[1;32m     74\u001b[0m )\n\u001b[1;32m     75\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m     77\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/langchain/llms/openai.py:158\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m    156\u001b[0m _keys \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mcompletion_tokens\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mprompt_tokens\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtotal_tokens\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m    157\u001b[0m \u001b[39mfor\u001b[39;00m _prompts \u001b[39min\u001b[39;00m sub_prompts:\n\u001b[0;32m--> 158\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(prompt\u001b[39m=\u001b[39;49m_prompts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    159\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    160\u001b[0m     _keys_to_use \u001b[39m=\u001b[39m _keys\u001b[39m.\u001b[39mintersection(response[\u001b[39m\"\u001b[39m\u001b[39musage\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/api_requestor.py:227\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    207\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    208\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    216\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    217\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    218\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    219\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    226\u001b[0m     )\n\u001b[0;32m--> 227\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    228\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/api_requestor.py:620\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    613\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    614\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    615\u001b[0m         )\n\u001b[1;32m    616\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    617\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    619\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 620\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    623\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    624\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    625\u001b[0m         ),\n\u001b[1;32m    626\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    627\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/openai/api_requestor.py:680\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    678\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    679\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    681\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    682\u001b[0m     )\n\u001b[1;32m    683\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 4097 tokens, however you requested 4671 tokens (4415 in your prompt; 256 for the completion). Please reduce your prompt; or completion length."
     ]
    }
   ],
   "source": [
    "%dotenv\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import CharacterTextSplitter, NLTKTextSplitter, SpacyTextSplitter\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "chain = LLMChain(llm=llm, prompt=podcast_summary_prompt)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for podcast in podcasts_to_analyze:\n",
    "  text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=1000, chunk_overlap=20)\n",
    "  text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "  texts = text_splitter.split_text(podcast)\n",
    "  print(len(texts))\n",
    "  results = []  \n",
    "  \n",
    "  for text in texts:\n",
    "    subset = chain.run(text)\n",
    "    results.append(subset)\n",
    "\n",
    "  all_results.append(results.join('. '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "400306f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-10 years since startup leader's creation\n",
      "-evaluatement of investments\n",
      "-focus on VCs\n",
      "-comment on national conversation around Twitter, combinator\n",
      "\n",
      "- Sam Alvin, a startup owner, talks about his experience with Hermias, a new company that wants to go at 5 times the speed of sound.\n",
      "- He is excited about the potential of fusion and solar and wind for Earth and the challenges that they face.\n",
      "- He also talks about his other investments in the podcast and the importance of sustainability.\n",
      "- The podcast ends with a discussion of World Coin, a company that Sam Alvin wants to scan your eyeball in exchange for cryptocurrency.\n",
      "\n",
      "-The company has to re-found its system, and it needs a CEO\n",
      "-Gary is an excellent CEO\n",
      "-He has been investing for 20 years and is now leaving the company\n",
      "-The company needs a CEO, and Gary is a great one\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
